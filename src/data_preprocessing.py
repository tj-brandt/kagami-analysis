# /src/data_preprocessing.py

"""
Data Preprocessing and De-identification Pipeline (Documentation Script)
======================================================================

!!! IMPORTANT NOTE FOR PUBLIC REPOSITORY USERS !!!

This script is provided for documentation and transparency purposes only.
It details the exact process used to generate the final, de-identified public
datasets from the original raw, private data sources.

*** THIS SCRIPT CANNOT BE RUN ON THE PUBLIC REPOSITORY ***

It requires access to sensitive raw data files (e.g., raw JSONL logs,
Qualtrics exports with PII, and Prolific demographic data) that have been
withheld to protect participant privacy, in accordance with the IRB protocol.

The final, de-identified output files generated by this script are already
included in the `/data/` directory of the public repository. All other public
analysis scripts (`confirmatory_analysis.py`, etc.) are designed to run on
those safe, public files.
"""

import pandas as pd
import json
import os
import logging
import numpy as np

# --- 1. CONFIGURATION ---
np.random.seed(42)

# >>>>> PRIVATE FILE PATHS (For Documentation Only) <<<<<
# These paths point to raw data sources that are not publicly available.
LOG_DIR = '../data/raw/logs/complete/'
QUALTRICS_FILE = '../data/raw/qualtrics.csv'
PROLIFIC_FILE = '../data/raw/prolific.csv'
TURNBYTURN_LIWC_FILE = '../data/processed/turnbyturn_liwc.csv'
OBJECTIVE_LSM_FILE = '../data/processed/objective_lsm.csv'
CODED_PROMPTS_FILE = '../data/processed/generated_prompts_coded.csv'

# >>>>> PUBLIC OUTPUT PATHS (Generated by this script) <<<<<
PUBLIC_OUTPUT_DIR = '../data'
PUBLIC_ANALYSIS_FILE = os.path.join(PUBLIC_OUTPUT_DIR, 'analysis_dataset_deidentified.csv')
PUBLIC_CHAT_METRICS_FILE = os.path.join(PUBLIC_OUTPUT_DIR, 'chat_metrics_derived.csv')
PUBLIC_PROMPTS_FILE = os.path.join(PUBLIC_OUTPUT_DIR, 'generated_prompts_coded_deidentified.csv')

# Setup logging and directories
os.makedirs(PUBLIC_OUTPUT_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- 2. LOGS-FIRST PARSING (Build the Source of Truth) ---
def parse_logs_for_completed_participants(log_dir):
    """
    Parses raw JSONL logs to identify all participants who completed the study.
    This serves as the primary filter for all other data sources.
    """
    logging.info(f"Parsing JSONL logs from: {log_dir}")
    log_data = []
    log_files = [f for f in os.listdir(log_dir) if f.endswith('.jsonl')]

    for filename in log_files:
        participant_id = filename.split('_')[1]
        try:
            with open(os.path.join(log_dir, filename), 'r') as f:
                first_line = next(f)
                log_entry = json.loads(first_line)
                condition_info = log_entry.get('backend_confirmed_condition_obj', {})
                avatar_type = condition_info.get('avatarType', 'none')
                lsm_type = 'adaptive' if condition_info.get('lsm', False) else 'static'
                log_data.append({'participant_id': participant_id, 'avatar_type_raw': avatar_type, 'lsm_type_raw': lsm_type})
        except (StopIteration, json.JSONDecodeError) as e:
            logging.warning(f"Could not parse metadata from log file: {filename}. Skipping. Error: {e}")

    logs_df = pd.DataFrame(log_data)
    logging.info(f"Identified {len(logs_df)} unique completed participants from logs.")
    return logs_df


# --- 3. DATA CLEANING AND MERGING ---
def process_and_merge_data(logs_df):
    """
    Loads, cleans, and merges raw Qualtrics and Prolific data, filtering
    to include only the completed participants identified from the logs.
    """
    completed_pids = logs_df['participant_id'].unique().tolist()

    # Process Qualtrics Data
    qualtrics_df = pd.read_csv(QUALTRICS_FILE, header=0, skiprows=[1, 2]).rename(columns={'PROLIFIC_PID': 'participant_id'})
    qualtrics_df = qualtrics_df[qualtrics_df['participant_id'].isin(completed_pids)].copy()
    
    # Convert text-based Likert scales to numeric values
    scale_maps = { 'Strongly disagree': 1, 'Disagree': 2, 'Neutral': 3, 'Neither agree nor disagree': 3, 'Agree': 4, 'Strongly agree': 5 }
    for col in qualtrics_df.columns:
        if qualtrics_df[col].dtype == 'object':
            qualtrics_df[col] = qualtrics_df[col].replace(scale_maps)
            qualtrics_df[col] = pd.to_numeric(qualtrics_df[col], errors='coerce')
    
    # Process Prolific Data
    prolific_df = pd.read_csv(PROLIFIC_FILE).rename(columns={'Participant id': 'participant_id'})
    prolific_df = prolific_df[prolific_df['participant_id'].isin(completed_pids)].copy()

    # Merge all data sources
    final_df = pd.merge(logs_df, qualtrics_df, on='participant_id', how='left')
    final_df = pd.merge(final_df, prolific_df, on='participant_id', how='left')
    logging.info("Successfully merged log, Qualtrics, and Prolific data sources.")
    return final_df


# --- 4. DE-IDENTIFICATION AND PUBLIC FILE CREATION ---
def create_public_analysis_file(df):
    """
    Creates the main de-identified public dataset by removing PII, dropping raw
    demographics, and applying k-anonymity techniques (binning, collapsing).
    """
    logging.info("Creating main public analysis file: analysis_dataset_deidentified.csv")
    df_public = df.copy()

    # Drop columns containing free text or direct identifiers
    columns_to_drop = [col for col in df_public.columns if col.startswith('OE') or col in ['Location', 'IPAddress']]
    df_public.drop(columns=columns_to_drop, inplace=True, errors='ignore')

    # Bin age for k-anonymity
    age_bins = [18, 25, 35, 45, 55, 65, 100]
    age_labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']
    df_public['age_binned'] = pd.cut(df_public['Age'], bins=age_bins, labels=age_labels, right=False)

    # Collapse gender and race/ethnicity categories
    df_public['gender_collapsed'] = df_public['Sex'].replace({'Female': 'Woman', 'Male': 'Man'}).fillna('Other/Unknown')
    df_public['race_ethnicity_collapsed'] = df_public['Ethnicity simplified'].where(df_public['Ethnicity simplified'].isin(['White', 'Black', 'Asian', 'Hispanic']), 'Mixed/Other')

    # Select and reorder final columns for the public file
    final_public_cols = ['participant_id', 'avatar_type_raw', 'lsm_type_raw', 'age_binned', 'gender_collapsed', 'race_ethnicity_collapsed'] + \
                        [col for col in df_public.columns if col.startswith(('MC', 'CDV', 'AP', 'WB'))]
    df_public = df_public[final_public_cols]
    
    df_public.to_csv(PUBLIC_ANALYSIS_FILE, index=False)
    logging.info(f"Saved de-identified main analysis file to: {PUBLIC_ANALYSIS_FILE}")

def create_public_chat_metrics_file():
    """
    Creates a public file of derived, non-sensitive chat metrics by aggregating
    per-turn LIWC scores and objective LSM data. All raw text is removed.
    """
    logging.info("Creating derived public chat metrics file: chat_metrics_derived.csv")
    
    # Aggregate LIWC data (numeric scores only)
    df_liwc = pd.read_csv(TURNBYTURN_LIWC_FILE)
    df_liwc_agg = df_liwc.groupby('participant_id').mean(numeric_only=True).reset_index()

    # Aggregate objective LSM data
    df_lsm = pd.read_csv(OBJECTIVE_LSM_FILE)
    df_lsm_agg = df_lsm[df_lsm['Person'] == 'user'].groupby('participant_id')['LSM'].mean().reset_index().rename(columns={'LSM': 'objective_lsm'})
    
    # Merge and save
    df_public = pd.merge(df_liwc_agg, df_lsm_agg, on='participant_id', how='left')
    df_public.to_csv(PUBLIC_CHAT_METRICS_FILE, index=False)
    logging.info(f"Saved derived chat metrics file to: {PUBLIC_CHAT_METRICS_FILE}")

def create_public_prompts_file():
    """
    Creates a public file of coded avatar prompt themes, stripping all raw
    user-generated text to protect privacy.
    """
    logging.info("Creating de-identified coded prompts file: generated_prompts_coded_deidentified.csv")
    df_prompts = pd.read_csv(CODED_PROMPTS_FILE)
    
    # Drop columns with raw text
    df_public = df_prompts.drop(columns=['avatar_prompt', 'notes'], errors='ignore')
    
    df_public.to_csv(PUBLIC_PROMPTS_FILE, index=False)
    logging.info(f"Saved coded prompts file to: {PUBLIC_PROMPTS_FILE}")


# --- 5. MAIN EXECUTION BLOCK ---
def main():
    """Main function to orchestrate the entire preprocessing and de-identification pipeline."""
    logging.info("--- Starting Data Preprocessing and De-identification ---")
    
    # Step 1: Identify the final cohort of participants from raw logs
    logs_df = parse_logs_for_completed_participants(LOG_DIR)
    
    # Step 2: Merge and clean all raw data for this cohort
    final_df = process_and_merge_data(logs_df)
    
    # Step 3: Create all final, de-identified public data files
    create_public_analysis_file(final_df)
    create_public_chat_metrics_file()
    create_public_prompts_file()
    
    logging.info("\n--- All public data files have been generated successfully. ---")

if __name__ == "__main__":
    print("This script documents the private data preprocessing and de-identification pipeline.")
    print("It requires access to sensitive raw data and will not run on the public repository.")
    # To run this script, you would need the private data files and uncomment the following line:
    # main()